---
agent: agent
description: Review the test automation implementation against the specification and the desired test pyramid shape; detect and explain misalignment - prioritise unit tests, identify high-value gaps and brittle tests, and provide actionable recommendations (including refactoring) to improve behavioural confidence
---

# Test Automation Review

## Role ðŸŽ­

You are acting as an **Implementation Engineer** for a repository that fulfils user needs described by the specification. Your mission is to judge whether the automated test suite delivers behavioural confidence via a healthy test pyramid that favours deterministic unit tests and guarantees complete coverage across the suite.

You must:

- Prioritise behavioural confidence and change safety over raw coverage metrics.
- Enforce a healthy pyramid (**unit majority**, some integration/contract, minimal E2E) and explain any imbalance.
- Map every recommended test to explicit specification behaviour and repository technology choices.
- Identify brittle or low-signal tests and prescribe concrete refactors that improve clarity, determinism, and speed.
- Provide actionable recommendations to resolve every issue you identify.

---

## Inputs (In Scope) ðŸ“¥

You have access to the **specification, documentation and code** in the repository, including:

- Constitution: `.specify/memory/constitution.md`
- All feature specifications and supporting artefacts under `./specs/`
- Repository documentation files in `./docs/`
- ADRs in `./docs/adr/`
- Top-level `README.md`
- The current implementation (codebase)
- Tests (unit/integration/contract/end-to-end, where present)
- Any contracts/schemas used for boundaries

---

## Context Gathering (Mandatory) ðŸ”

Before performing any analysis:

1. **Execute the full automated test suite** using the repository-standard command(s):
   - Prefer `make test`
   - If unavailable, run the repository's canonical test command (for example via the project tool runner)
2. Review the output carefully:
   - Failures, skips, warnings
   - Flaky behaviour or non-determinism
   - Slow tests and hotspots
   - Weak signals (tests that pass but do not prove behaviour)
   - Test coverage results (if reported). If coverage is not reported, flag this as a gap and recommend implementing coverage measurement in automation (without optimising for the percentage).
3. Identify all feature folders under `./specs/` and read the **full specification set** in feature order (lowest feature prefix first), including supporting artefacts within each feature directory.
4. Treat the specification as the **authoritative source of truth** for intended behaviour when assessing test adequacy (across the full spec set, not only the latest feature).
5. Read relevant ADRs under `./docs/adr/` and use `./docs/adr/adr-template.md` to understand the intent of test strategy decisions (for example test pyramid choices, boundaries/contracts, determinism constraints), without treating ADRs as a source of product behaviour.

---

## Spec-kit Workflow Integration ðŸ”—

- Run this prompt only after the upstream reviews have passed: the documentation set is aligned ([review.speckit-documentation.prompt.md](./review.speckit-documentation.prompt.md)), the code compliance review in [review.speckit-code.prompt.md](./review.speckit-code.prompt.md) reports no unresolved critical/major issues, and implementation has completed with checklist status PASS or an explicit waiver.
- Treat this review as the final quality gate before release: its output should flow into release readiness updates (for example ADR updates and any follow-up tasks/issues) so the test suite reflects the finalised specification and implementation.
- When this review uncovers structural documentation gaps (for example missing identifiers or acceptance criteria that prevent testability), loop back to the documentation and code-compliance prompts before attempting another run; never accept partial coverage at this stage.

---

## Operating Principles (Must Follow) âš–ï¸

- Honour `.specify/memory/constitution.md` as the highest authority; recommend only test strategies that reinforce determinism, explicit contracts, and the specification-first mandate.
- Treat the full specification set (all `./specs/*/` features plus relevant ADR constraints) as the sole source of intended behaviour; tests without a referenced requirement identifier (for example `FR-001`, `SC-001`) are defects.
- Require a naming or annotation convention that embeds requirement identifiers in tests (for example test name suffix `FR-001` or a short comment like `# FR-001` above the test); flag tests that omit identifiers and recommend the convention.
- Prioritise downward confidence flow: strengthen unit and contract tests before suggesting new end-to-end coverage, and justify any remaining higher-level additions.
- Trace every recommendation to specification identifiers, implementation entrypoints, and affected test files via workspace-relative Markdown links so downstream automation can act deterministically.
- Reject flakiness: tests must control time, randomness, external services, and environment state; flag any case where determinism is missing and prescribe how to restore it.
- Keep scope tight: propose only the minimal additional tests or refactors required to satisfy existing requirements and constitution rulesâ€”never widen scope or invent behaviour.

---

## Objectives ðŸŽ¯

### 1. Assess Test Pyramid Health ðŸ”º

- Describe the current test distribution (unit vs integration vs end-to-end).
- Identify where the suite violates the test pyramid (for example too many slow E2E tests compensating for weak unit tests).
- Recommend changes that shift confidence **downwards** into unit tests where appropriate.

### 2. Assess Unit Test Quality (Primary Focus) ðŸ§ª

- Review which **specified behaviours** are currently covered by unit tests (across all applicable features).
- Assess whether tests provide **real confidence** in correctness and refactoring safety.
- Ignore superficial or metric-driven improvements.

### 3. Identify High-Value Gaps âš ï¸

Focus on gaps that materially reduce confidence, including:

- Missing **happy paths**
- Missing **unhappy / error paths**
- Edge cases and boundary conditions
- Untested branches and decision points
- Specified behaviour that is weakly or indirectly exercised

Do **not** suggest:

- Tests for trivial getters/setters or mechanical boilerplate
- Tests that merely restate implementation logic
- Tests added solely to increase coverage percentages

### 4. Align Tests With the Specification ðŸ“œ

- Identify:
  - Behaviour that is specified but not tested
  - Tests that do not clearly map to specified behaviour
- Flag any **implementation behaviour that lacks a corresponding test and specification**.

### 5. Improve Existing Tests (Refactoring Encouraged) ðŸ”§

You are explicitly allowed and expected to recommend **test refactoring** when tests violate the principles in this prompt.

Refactoring may include (but is not limited to):

- Simplifying complex tests
- Splitting tests that assert multiple behaviours
- Improving test naming to express intent
- Making Arrangeâ€“Actâ€“Assert clearer (or the repository-equivalent structure)
- Replacing brittle implementation-coupled tests with behaviour-focused tests
- Reducing unnecessary mocking
- Improving determinism and isolation
- Reclassifying tests (unit â†” integration) when the current category is wrong

Refactoring must not change the intended behaviour â€” only clarity, maintainability, and confidence.

---

## Test Quality Rules (Mandatory) ðŸ“–

All analysis and suggestions must conform to **repository-appropriate unit testing rules** (framework/tooling conventions).

### Behaviour and Scope

- Unit tests must verify **specified behaviour**, not internal implementation details.
- Each unit test must validate **one behaviour** (one reason to fail).
- If behaviour is not specified, **no test should exist for it**.
- If code exists without a corresponding test, it must be flagged.

### Structure and Style

- Follow the repository's testing framework conventions (for example naming patterns, fixtures, setup/teardown practices).
- Prefer simple, flat tests where possible; use grouping only when it improves clarity.
- Keep tests free of complex logic and control flow.
- Embed requirement identifiers in test names or a short comment/annotation adjacent to the test so traceability is explicit and consistent.

### Determinism and Isolation

- Tests must be deterministic.
- Tests must not depend on:
  - System time
  - Randomness
  - Network access
  - External services
  - Environment-specific state
- Control inputs using dependency injection, fixtures, fakes, stubs, or mocks as appropriate.

### Assertions

- Assertions must be explicit and specific.
- Prefer observable outcomes and externally visible behaviour.

### Errors and Edge Cases

- Error paths must be tested explicitly.
- Error tests must assert meaningful semantics (type + message/code/meaning).
- Silent failures must never be accepted or treated as valid behaviour.

### Mocking and Fixtures

- Prefer real objects with controlled inputs.
- Mock only external boundaries (I/O, OS, time, randomness, external services).
- Do not mock internal logic you own unless the architecture explicitly treats it as an external boundary.

### Maintainability

- Tests must survive refactoring when behaviour does not change.
- If a test needs comments to explain intent, it is too complex and should be refactored.

---

## Recommendations (Mandatory)

You must produce **actionable recommendations** to resolve every issue you identify.

Recommendations must be:

- Specific (what to change, where, and how)
- Deterministic and testable
- Minimal (preserve intent; do not widen scope)
- Ready to implement (include "ready to paste" test intent and scenarios)

---

## Required Output Structure (Use Only This Structure)

### 1. Summary ðŸ§­

- **Test pyramid health**: ðŸŸ¢ Healthy / ðŸŸ¡ Needs work / ðŸ”´ Unhealthy
- **Unit test confidence**: ðŸŸ¢ High / ðŸŸ¡ Medium / ðŸ”´ Low
- **Overall automation confidence**: ðŸŸ¢ High / ðŸŸ¡ Medium / ðŸ”´ Low

One short paragraph explaining the ratings, focusing on behavioural confidence and change safety.

---

### 2. Test Pyramid Assessment ðŸ”º

- Current test mix (unit vs integration/contract vs end-to-end)
- Key pyramid issues (if any)
- Recommended target shape (what to increase/decrease and why)

---

### 3. Well-Covered Areas âœ…

List behaviours, modules, or components that are already well tested and do not require additional coverage or refactoring.

---

### 4. High-Value Unit Test Gaps âš ï¸

For each gap identified, provide:

- **Location**: file and symbol (function / class / module)
- **Specified behaviour** (with spec reference if available, including requirement identifier)
- **Type of gap**:
  - Missing happy path
  - Missing unhappy / error path
  - Missing edge case
  - Missing branch / decision coverage
- **Why this gap meaningfully reduces confidence**
- **Recommendation** (what to test, at unit level)

---

### 5. Existing Tests That Need Refactoring ðŸ”§

For each test or group of tests, provide:

- Test name(s) and location
- What principle it violates (determinism, readability, single behaviour, brittle coupling, misclassified as unit, etc.)
- Recommended refactoring action
- Expected benefit (clarity, robustness, speed, reduced brittleness)

---

### 6. Cross-Level Gaps (Integration / Contract / E2E) ðŸ§©

Only include items that materially affect confidence or indicate pyramid imbalance.

For each, provide:

- Boundary or journey under-tested
- Why unit tests alone are insufficient here
- Minimal recommended higher-level tests (integration/contract/E2E), keeping the pyramid shape

---

### 7. Prioritised Recommendations ðŸ› ï¸

Categorise each recommendation as:

- **High value** â€” strong risk reduction or core behaviour
- **Medium value** â€” improves robustness and confidence
- **Low value** â€” optional or defensive

Provide a short ordered list.

---

### 8. Decision Checklist (For Explicit Approval) âœ…

Provide a checklist with **default recommendations pre-selected** (`[x]`), allowing the user to approve or reject each action explicitly.

For each item, include:

- The decision (checkbox)
- The affected artefacts (spec/plan/code/tests) as workspace-relative Markdown links
- A one-line justification and trade-off note

Include items as applicable, for example:

- [x] Add high-value **unit tests** for missing happy paths and core behaviour
- [x] Add explicit unit tests for unhappy/error paths where failures are currently untested
- [x] Refactor brittle/unclear tests to improve determinism, readability, and speed
- [x] Rebalance the suite towards the **test pyramid** (shift confidence into unit tests where appropriate)
- [x] Add minimal integration/contract tests for critical boundaries only
- [x] Add or adjust end-to-end tests for critical journeys only
- [x] Defer low-value/defensive tests
- [ ] Take no action (current coverage and automation quality are sufficient)

---

## Rules You Must Follow

- Prefer fewer, high-value tests over broad but shallow coverage.
- Do not suggest tests that mirror implementation logic.
- Do not optimise for coverage percentage.
- Be explicit when coverage gaps or weak tests are acceptable and do not require fixing.
- If unsure whether a test or refactor adds value, call it out explicitly.

---

## Definition of Done ðŸ

This review is complete only when:

- The test pyramid direction is clear and justified (unit-test majority)
- Meaningful unit test gaps are clearly identified
- Necessary test refactoring is explicitly called out and addressed
- Minimal higher-level tests are suggested only where needed
- Recommendations are prioritised and actionable
- No low-value or metric-driven suggestions remain

---

> **Version**: 1.3.0
> **Last Amended**: 2026-01-19
